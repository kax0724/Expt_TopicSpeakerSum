{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from utils import calculate_rouge, chunks, parse_numeric_n_bool_cl_kwargs, use_task_specific_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./output/2020-12-15-01-44-14/best_tfmr were not used when initializing PegasusForConditionalGeneration: ['model.encoder.embed_speaker.weight']\n",
      "- This IS expected if you are initializing PegasusForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PegasusForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = './output/2020-12-15-01-44-14/best_tfmr'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(os.path.join(model_name,\"pytorch_model.bin\"), map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('model.encoder.embed_speaker.weight' in state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.4873,  0.4773,  0.6709,  ..., -0.0336,  1.0234, -1.6810],\n",
       "        [-0.7433,  2.3412,  0.5041,  ...,  0.5177, -0.1096, -0.6892],\n",
       "        ...,\n",
       "        [ 1.1140,  0.2462,  1.6674,  ..., -0.0325, -0.1390, -0.9116],\n",
       "        [-0.2555,  2.9544, -1.4787,  ..., -0.4960,  0.4160,  1.1340],\n",
       "        [ 1.3796, -1.3169,  0.1431,  ...,  0.2244, -1.1140, -0.1873]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['model.encoder.embed_speaker.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_embed_encoder import BartEncoderWithSpeakerEmbedding\n",
    "speaker_encoder = BartEncoderWithSpeakerEmbedding(model.config, model.model.shared, use_turn_embeds=False).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight\n",
      "embed_positions.weight\n",
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]], device='cuda:0')\n",
      "layers.0.self_attn.k_proj.weight\n",
      "layers.0.self_attn.k_proj.bias\n",
      "layers.0.self_attn.v_proj.weight\n",
      "layers.0.self_attn.v_proj.bias\n",
      "layers.0.self_attn.q_proj.weight\n",
      "layers.0.self_attn.q_proj.bias\n",
      "layers.0.self_attn.out_proj.weight\n",
      "layers.0.self_attn.out_proj.bias\n",
      "layers.0.self_attn_layer_norm.weight\n",
      "layers.0.self_attn_layer_norm.bias\n",
      "layers.0.fc1.weight\n",
      "layers.0.fc1.bias\n",
      "layers.0.fc2.weight\n",
      "layers.0.fc2.bias\n",
      "layers.0.final_layer_norm.weight\n",
      "layers.0.final_layer_norm.bias\n",
      "layers.1.self_attn.k_proj.weight\n",
      "layers.1.self_attn.k_proj.bias\n",
      "layers.1.self_attn.v_proj.weight\n",
      "layers.1.self_attn.v_proj.bias\n",
      "layers.1.self_attn.q_proj.weight\n",
      "layers.1.self_attn.q_proj.bias\n",
      "layers.1.self_attn.out_proj.weight\n",
      "layers.1.self_attn.out_proj.bias\n",
      "layers.1.self_attn_layer_norm.weight\n",
      "layers.1.self_attn_layer_norm.bias\n",
      "layers.1.fc1.weight\n",
      "layers.1.fc1.bias\n",
      "layers.1.fc2.weight\n",
      "layers.1.fc2.bias\n",
      "layers.1.final_layer_norm.weight\n",
      "layers.1.final_layer_norm.bias\n",
      "layers.2.self_attn.k_proj.weight\n",
      "layers.2.self_attn.k_proj.bias\n",
      "layers.2.self_attn.v_proj.weight\n",
      "layers.2.self_attn.v_proj.bias\n",
      "layers.2.self_attn.q_proj.weight\n",
      "layers.2.self_attn.q_proj.bias\n",
      "layers.2.self_attn.out_proj.weight\n",
      "layers.2.self_attn.out_proj.bias\n",
      "layers.2.self_attn_layer_norm.weight\n",
      "layers.2.self_attn_layer_norm.bias\n",
      "layers.2.fc1.weight\n",
      "layers.2.fc1.bias\n",
      "layers.2.fc2.weight\n",
      "layers.2.fc2.bias\n",
      "layers.2.final_layer_norm.weight\n",
      "layers.2.final_layer_norm.bias\n",
      "layers.3.self_attn.k_proj.weight\n",
      "layers.3.self_attn.k_proj.bias\n",
      "layers.3.self_attn.v_proj.weight\n",
      "layers.3.self_attn.v_proj.bias\n",
      "layers.3.self_attn.q_proj.weight\n",
      "layers.3.self_attn.q_proj.bias\n",
      "layers.3.self_attn.out_proj.weight\n",
      "layers.3.self_attn.out_proj.bias\n",
      "layers.3.self_attn_layer_norm.weight\n",
      "layers.3.self_attn_layer_norm.bias\n",
      "layers.3.fc1.weight\n",
      "layers.3.fc1.bias\n",
      "layers.3.fc2.weight\n",
      "layers.3.fc2.bias\n",
      "layers.3.final_layer_norm.weight\n",
      "layers.3.final_layer_norm.bias\n",
      "layers.4.self_attn.k_proj.weight\n",
      "layers.4.self_attn.k_proj.bias\n",
      "layers.4.self_attn.v_proj.weight\n",
      "layers.4.self_attn.v_proj.bias\n",
      "layers.4.self_attn.q_proj.weight\n",
      "layers.4.self_attn.q_proj.bias\n",
      "layers.4.self_attn.out_proj.weight\n",
      "layers.4.self_attn.out_proj.bias\n",
      "layers.4.self_attn_layer_norm.weight\n",
      "layers.4.self_attn_layer_norm.bias\n",
      "layers.4.fc1.weight\n",
      "layers.4.fc1.bias\n",
      "layers.4.fc2.weight\n",
      "layers.4.fc2.bias\n",
      "layers.4.final_layer_norm.weight\n",
      "layers.4.final_layer_norm.bias\n",
      "layers.5.self_attn.k_proj.weight\n",
      "layers.5.self_attn.k_proj.bias\n",
      "layers.5.self_attn.v_proj.weight\n",
      "layers.5.self_attn.v_proj.bias\n",
      "layers.5.self_attn.q_proj.weight\n",
      "layers.5.self_attn.q_proj.bias\n",
      "layers.5.self_attn.out_proj.weight\n",
      "layers.5.self_attn.out_proj.bias\n",
      "layers.5.self_attn_layer_norm.weight\n",
      "layers.5.self_attn_layer_norm.bias\n",
      "layers.5.fc1.weight\n",
      "layers.5.fc1.bias\n",
      "layers.5.fc2.weight\n",
      "layers.5.fc2.bias\n",
      "layers.5.final_layer_norm.weight\n",
      "layers.5.final_layer_norm.bias\n",
      "layers.6.self_attn.k_proj.weight\n",
      "layers.6.self_attn.k_proj.bias\n",
      "layers.6.self_attn.v_proj.weight\n",
      "layers.6.self_attn.v_proj.bias\n",
      "layers.6.self_attn.q_proj.weight\n",
      "layers.6.self_attn.q_proj.bias\n",
      "layers.6.self_attn.out_proj.weight\n",
      "layers.6.self_attn.out_proj.bias\n",
      "layers.6.self_attn_layer_norm.weight\n",
      "layers.6.self_attn_layer_norm.bias\n",
      "layers.6.fc1.weight\n",
      "layers.6.fc1.bias\n",
      "layers.6.fc2.weight\n",
      "layers.6.fc2.bias\n",
      "layers.6.final_layer_norm.weight\n",
      "layers.6.final_layer_norm.bias\n",
      "layers.7.self_attn.k_proj.weight\n",
      "layers.7.self_attn.k_proj.bias\n",
      "layers.7.self_attn.v_proj.weight\n",
      "layers.7.self_attn.v_proj.bias\n",
      "layers.7.self_attn.q_proj.weight\n",
      "layers.7.self_attn.q_proj.bias\n",
      "layers.7.self_attn.out_proj.weight\n",
      "layers.7.self_attn.out_proj.bias\n",
      "layers.7.self_attn_layer_norm.weight\n",
      "layers.7.self_attn_layer_norm.bias\n",
      "layers.7.fc1.weight\n",
      "layers.7.fc1.bias\n",
      "layers.7.fc2.weight\n",
      "layers.7.fc2.bias\n",
      "layers.7.final_layer_norm.weight\n",
      "layers.7.final_layer_norm.bias\n",
      "layers.8.self_attn.k_proj.weight\n",
      "layers.8.self_attn.k_proj.bias\n",
      "layers.8.self_attn.v_proj.weight\n",
      "layers.8.self_attn.v_proj.bias\n",
      "layers.8.self_attn.q_proj.weight\n",
      "layers.8.self_attn.q_proj.bias\n",
      "layers.8.self_attn.out_proj.weight\n",
      "layers.8.self_attn.out_proj.bias\n",
      "layers.8.self_attn_layer_norm.weight\n",
      "layers.8.self_attn_layer_norm.bias\n",
      "layers.8.fc1.weight\n",
      "layers.8.fc1.bias\n",
      "layers.8.fc2.weight\n",
      "layers.8.fc2.bias\n",
      "layers.8.final_layer_norm.weight\n",
      "layers.8.final_layer_norm.bias\n",
      "layers.9.self_attn.k_proj.weight\n",
      "layers.9.self_attn.k_proj.bias\n",
      "layers.9.self_attn.v_proj.weight\n",
      "layers.9.self_attn.v_proj.bias\n",
      "layers.9.self_attn.q_proj.weight\n",
      "layers.9.self_attn.q_proj.bias\n",
      "layers.9.self_attn.out_proj.weight\n",
      "layers.9.self_attn.out_proj.bias\n",
      "layers.9.self_attn_layer_norm.weight\n",
      "layers.9.self_attn_layer_norm.bias\n",
      "layers.9.fc1.weight\n",
      "layers.9.fc1.bias\n",
      "layers.9.fc2.weight\n",
      "layers.9.fc2.bias\n",
      "layers.9.final_layer_norm.weight\n",
      "layers.9.final_layer_norm.bias\n",
      "layers.10.self_attn.k_proj.weight\n",
      "layers.10.self_attn.k_proj.bias\n",
      "layers.10.self_attn.v_proj.weight\n",
      "layers.10.self_attn.v_proj.bias\n",
      "layers.10.self_attn.q_proj.weight\n",
      "layers.10.self_attn.q_proj.bias\n",
      "layers.10.self_attn.out_proj.weight\n",
      "layers.10.self_attn.out_proj.bias\n",
      "layers.10.self_attn_layer_norm.weight\n",
      "layers.10.self_attn_layer_norm.bias\n",
      "layers.10.fc1.weight\n",
      "layers.10.fc1.bias\n",
      "layers.10.fc2.weight\n",
      "layers.10.fc2.bias\n",
      "layers.10.final_layer_norm.weight\n",
      "layers.10.final_layer_norm.bias\n",
      "layers.11.self_attn.k_proj.weight\n",
      "layers.11.self_attn.k_proj.bias\n",
      "layers.11.self_attn.v_proj.weight\n",
      "layers.11.self_attn.v_proj.bias\n",
      "layers.11.self_attn.q_proj.weight\n",
      "layers.11.self_attn.q_proj.bias\n",
      "layers.11.self_attn.out_proj.weight\n",
      "layers.11.self_attn.out_proj.bias\n",
      "layers.11.self_attn_layer_norm.weight\n",
      "layers.11.self_attn_layer_norm.bias\n",
      "layers.11.fc1.weight\n",
      "layers.11.fc1.bias\n",
      "layers.11.fc2.weight\n",
      "layers.11.fc2.bias\n",
      "layers.11.final_layer_norm.weight\n",
      "layers.11.final_layer_norm.bias\n",
      "layers.12.self_attn.k_proj.weight\n",
      "layers.12.self_attn.k_proj.bias\n",
      "layers.12.self_attn.v_proj.weight\n",
      "layers.12.self_attn.v_proj.bias\n",
      "layers.12.self_attn.q_proj.weight\n",
      "layers.12.self_attn.q_proj.bias\n",
      "layers.12.self_attn.out_proj.weight\n",
      "layers.12.self_attn.out_proj.bias\n",
      "layers.12.self_attn_layer_norm.weight\n",
      "layers.12.self_attn_layer_norm.bias\n",
      "layers.12.fc1.weight\n",
      "layers.12.fc1.bias\n",
      "layers.12.fc2.weight\n",
      "layers.12.fc2.bias\n",
      "layers.12.final_layer_norm.weight\n",
      "layers.12.final_layer_norm.bias\n",
      "layers.13.self_attn.k_proj.weight\n",
      "layers.13.self_attn.k_proj.bias\n",
      "layers.13.self_attn.v_proj.weight\n",
      "layers.13.self_attn.v_proj.bias\n",
      "layers.13.self_attn.q_proj.weight\n",
      "layers.13.self_attn.q_proj.bias\n",
      "layers.13.self_attn.out_proj.weight\n",
      "layers.13.self_attn.out_proj.bias\n",
      "layers.13.self_attn_layer_norm.weight\n",
      "layers.13.self_attn_layer_norm.bias\n",
      "layers.13.fc1.weight\n",
      "layers.13.fc1.bias\n",
      "layers.13.fc2.weight\n",
      "layers.13.fc2.bias\n",
      "layers.13.final_layer_norm.weight\n",
      "layers.13.final_layer_norm.bias\n",
      "layers.14.self_attn.k_proj.weight\n",
      "layers.14.self_attn.k_proj.bias\n",
      "layers.14.self_attn.v_proj.weight\n",
      "layers.14.self_attn.v_proj.bias\n",
      "layers.14.self_attn.q_proj.weight\n",
      "layers.14.self_attn.q_proj.bias\n",
      "layers.14.self_attn.out_proj.weight\n",
      "layers.14.self_attn.out_proj.bias\n",
      "layers.14.self_attn_layer_norm.weight\n",
      "layers.14.self_attn_layer_norm.bias\n",
      "layers.14.fc1.weight\n",
      "layers.14.fc1.bias\n",
      "layers.14.fc2.weight\n",
      "layers.14.fc2.bias\n",
      "layers.14.final_layer_norm.weight\n",
      "layers.14.final_layer_norm.bias\n",
      "layers.15.self_attn.k_proj.weight\n",
      "layers.15.self_attn.k_proj.bias\n",
      "layers.15.self_attn.v_proj.weight\n",
      "layers.15.self_attn.v_proj.bias\n",
      "layers.15.self_attn.q_proj.weight\n",
      "layers.15.self_attn.q_proj.bias\n",
      "layers.15.self_attn.out_proj.weight\n",
      "layers.15.self_attn.out_proj.bias\n",
      "layers.15.self_attn_layer_norm.weight\n",
      "layers.15.self_attn_layer_norm.bias\n",
      "layers.15.fc1.weight\n",
      "layers.15.fc1.bias\n",
      "layers.15.fc2.weight\n",
      "layers.15.fc2.bias\n",
      "layers.15.final_layer_norm.weight\n",
      "layers.15.final_layer_norm.bias\n",
      "layer_norm.weight\n",
      "layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.model.encoder.named_parameters():\n",
    "    print(name)\n",
    "    if name==\"embed_positions.weight\":\n",
    "        print(param==model.model.encoder.embed_positions.weight)\n",
    "    speaker_encoder.state_dict()[name][:] = param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_encoder.embed_positions.weight == model.model.encoder.embed_positions.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_encoder.embed_speaker.weight = torch.nn.Parameter(state_dict['model.encoder.embed_speaker.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.encoder = speaker_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.4873,  0.4773,  0.6709,  ..., -0.0336,  1.0234, -1.6810],\n",
       "        [-0.7433,  2.3412,  0.5041,  ...,  0.5177, -0.1096, -0.6892],\n",
       "        ...,\n",
       "        [ 1.1140,  0.2462,  1.6674,  ..., -0.0325, -0.1390, -0.9116],\n",
       "        [-0.2555,  2.9544, -1.4787,  ..., -0.4960,  0.4160,  1.1340],\n",
       "        [ 1.3796, -1.3169,  0.1431,  ...,  0.2244, -1.1140, -0.1873]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_encoder.embed_speaker.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
